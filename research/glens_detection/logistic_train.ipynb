{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "future-nancy",
   "metadata": {},
   "source": [
    "# GLENS - LOGISTIC REGRESSION\n",
    "\n",
    "Project with Jim and Lantao.\n",
    "\n",
    "Control Simuations\n",
    "* 21 simulations 2010-2030\n",
    "* 3 of those continue to 2097 [1,2,3]\n",
    "\n",
    "GLENS Simulations\n",
    "* 20 simulations of 2020-2099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "linear-contest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONHASHSEED=99\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import random\n",
    "import xarray as xr\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "\n",
    "import time\n",
    "import os.path\n",
    "from os import path\n",
    "import subprocess\n",
    "import copy as copy\n",
    "import pickle\n",
    "\n",
    "# https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "# import innvestigate\n",
    "\n",
    "import seaborn as sns\n",
    "import palettable\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy as ct\n",
    "import cartopy.crs as ccrs\n",
    "import cmocean as cmocean\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import palettable\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "# from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "import experiment_settings\n",
    "\n",
    "# Plotting\n",
    "mpl.rcParams['figure.dpi']= 150\n",
    "dpiFig = 300.\n",
    "CL = 0.\n",
    "mapProj = ct.crs.EqualEarth(central_longitude = CL)\n",
    "\n",
    "# random numbers\n",
    "%env PYTHONHASHSEED=99\n",
    "np.random.seed(99)\n",
    "tf.random.set_seed(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c2bad7-383e-41a4-9e0c-758dcb3e0eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version = 1.22.2\n",
      "xarray version = 2022.3.0\n",
      "tensorflow version = 2.7.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"numpy version = {np.__version__}\")\n",
    "print(f\"xarray version = {xr.__version__}\")  \n",
    "print(f\"tensorflow version = {tf.__version__}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "architectural-uzbekistan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'var': 'T',\n",
       " 'n_train_val_test': (16, 4, 0),\n",
       " 'ens_seed': (3529, 4529, 5529, 6529, 7529),\n",
       " 'net_seed': (2222, 3333, 4444, 5555, 6666),\n",
       " 'land_only': True,\n",
       " 'remove_mean': False,\n",
       " 'network_type': 'logistic',\n",
       " 'hiddens': [1],\n",
       " 'dropout_rate': 0.0,\n",
       " 'ridge_param': 0.0,\n",
       " 'learning_rate': 0.001,\n",
       " 'n_epochs': 75,\n",
       " 'batch_size': 32,\n",
       " 'patience': 50,\n",
       " 'lr_epoch_bound': 150,\n",
       " 'exp_name': 'exp2a'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EXP_NAME = 'exp2a' #'exp0_tx90p' #'exp1_r95ptot' 'exp2_t'\n",
    "OVERWRITE_MODEL = True\n",
    "SAVE_DATA = False\n",
    "\n",
    "#-------------------------------------------------------\n",
    "settings = experiment_settings.get_settings(EXP_NAME)\n",
    "display(settings)\n",
    "\n",
    "fileDir = 'data/GLENS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c8f19fe-125a-44b2-8000-410eb6e8b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(settings[\"var\"]=='TX90p' or settings[\"var\"]=='TX10p' or settings[\"var\"]=='R95pTOT'):\n",
    "    extremeBool = True    \n",
    "else:\n",
    "    extremeBool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-meaning",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ML Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "minute-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_epoch_bound = settings[\"lr_epoch_bound\"]\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < lr_epoch_bound:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr #* tf.math.exp(-0.05)\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-greek",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wrong-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(X, labels, features):\n",
    "    y_pred = np.asarray(np.round(model.predict(X)),dtype='float')\n",
    "    y_pred_like = model.predict(X)\n",
    "#     y_pred_like = np.fromiter((row[index] for row, index in zip(y_pred_like, np.asarray(labels.flatten(),dtype='int'))), dtype='float')\n",
    "    \n",
    "    y_pred_like = y_pred_like.reshape((features.shape[0],features.shape[1]))\n",
    "    y_pred = y_pred.reshape((features.shape[0],features.shape[1]))\n",
    "    acc = np.asarray(np.asarray(np.equal(y_pred,labels),dtype='int32'),dtype='float') # convert True/False to zeros and ones\n",
    "    \n",
    "    # replace nans with nan\n",
    "    jnan = np.isnan(features[:,:,0,0])\n",
    "    y_pred_like[jnan] = np.nan\n",
    "    y_pred[jnan] = np.nan\n",
    "    acc[jnan] = np.nan\n",
    "\n",
    "    return y_pred_like, y_pred, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-pixel",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "numerous-feature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ..., nan, nan, nan], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mask = xr.open_dataset('data/GLENS/landSeaMask.r90x45.nc')\n",
    "# print(df_mask)\n",
    "landSeaMask = df_mask['TN10p'].values[0,0,:,:]\n",
    "lats = df_mask['lat']\n",
    "lons = df_mask['lon']\n",
    "df_mask.close()\n",
    "\n",
    "landSeaMask[np.isnan(landSeaMask)==False] = 1.\n",
    "landSeaMask[np.isnan(landSeaMask)==True] = np.nan\n",
    "landSeaMask = landSeaMask.reshape(len(lats)*len(lons),)\n",
    "landSeaMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "impressed-complement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 77, 45, 90)\n",
      "(20, 77, 45, 90)\n"
     ]
    }
   ],
   "source": [
    "if(extremeBool == False):\n",
    "    middletext = '.r90x45.shift.annual'\n",
    "\n",
    "#     print('** control has different months for the same year **')    \n",
    "    runType = 'control'\n",
    "    dsc = xr.open_mfdataset(fileDir + runType + '*.' + settings[\"var\"] + middletext + '.nc', concat_dim='ensmember', combine='nested')    \n",
    "    time_dsc = dsc['time.year']\n",
    "    \n",
    "    runType = 'feedback'\n",
    "    dsf = xr.open_mfdataset(fileDir + runType + '*.' + settings[\"var\"] + middletext + '.nc', concat_dim='ensmember', combine='nested')\n",
    "    time_dsf = dsf['time.year']\n",
    "    \n",
    "    if(settings[\"var\"]=='T'):\n",
    "        dsf = dsf.sel(lev=1000.)\n",
    "        dsc = dsc.sel(lev=1000.)    \n",
    "    \n",
    "else:    # get the extremes data\n",
    "    varFile = 'TREFHTMX'\n",
    "    if(settings[\"var\"]=='R95pTOT'):\n",
    "        varFile = 'PRECT'\n",
    "\n",
    "    middletext = '.swap.r90x45'\n",
    "    runType = 'control'\n",
    "    filename = fileDir + 'b.e15.B5505C5WCCML45BGCR.f09_g16.' + runType + '.extreme.cam.h3.' + varFile + '.20100101-20971231' + middletext + '.nc'\n",
    "    dsc = xr.open_dataset(filename)\n",
    "    dsc = dsc.transpose('ensemble', 'time', 'lat', 'lon')\n",
    "    time_dsc = np.arange(2010, 2098, 1)\n",
    "    \n",
    "    runType = 'feedback'\n",
    "    filename = fileDir + 'b.e15.B5505C5WCCML45BGCR.f09_g16.' + runType + '.extreme.cam.h3.' + varFile + '.20200101-20991231' + middletext + '.nc'\n",
    "    dsf = xr.open_dataset(filename)\n",
    "    dsf = dsf.transpose('ensemble', 'time', 'lat', 'lon')\n",
    "    time_dsf = np.arange(2020,2100,1)\n",
    "#------------------------------------------------------------------------\n",
    "# CONTROL\n",
    "itime = np.where((time_dsc>=2021) & (time_dsc<=2097))[0]    \n",
    "featuresC = dsc[settings[\"var\"]][:,itime,:,:].values.astype('float')\n",
    "time_dsc = time_dsc[itime]\n",
    "\n",
    "if(featuresC.shape[0]==21):\n",
    "    featuresC = featuresC[:-1,:,:,:]\n",
    "elif(featuresC.shape[0]==3):\n",
    "    filename = fileDir + 'b.e15.B5505C5WCCML45BGCR.f09_g16.' + 'control' + '.extreme.cam.h3.' + varFile + '.20210101-20301231' + middletext + '.nc'\n",
    "    dsc20 = xr.open_dataset(filename)\n",
    "    dsc20 = dsc20.transpose('ensemble', 'time', 'lat', 'lon')\n",
    "    featuresC20 = dsc20[settings[\"var\"]].values.astype('float')\n",
    "    hold = np.nan*np.empty((20,77,45,90))\n",
    "    hold[:3,:,:,:] = featuresC\n",
    "    hold[3:,:featuresC20.shape[1],:,:] = featuresC20[3:,:,:,:]\n",
    "    featuresC = copy.deepcopy(hold)\n",
    "#------------------------------------------------------------------------    \n",
    "# FEEDBACK\n",
    "itime = np.where((time_dsf>=2021) & (time_dsf<=2097))[0]   \n",
    "featuresF = dsf[settings[\"var\"]][:,itime,:,:].values.astype('float')\n",
    "time_dsf = time_dsf[itime]\n",
    "if(featuresF.shape[0]==21):\n",
    "    featuresF = featuresF[:-1,:,:,:]\n",
    "\n",
    "featuresC_raw = copy.deepcopy(featuresC)\n",
    "featuresF_raw = copy.deepcopy(featuresF)\n",
    "print(featuresC_raw.shape)\n",
    "print(featuresF_raw.shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-bandwidth",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "literary-island",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3529\n",
      "ensemble list = [ 1  2 16  6 10 14 11 15 13 12  8  3  5  9  7  4 17 18 19  0]\n",
      "\n",
      "training members = [ 1  2 16  6 10 14 11 15 13 12  8  3  5  9  7  4]\n",
      "validate members = [17 18 19  0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-08 12:14:30.417855: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/anaconda3/envs/env-noah/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1)                 4051      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,051\n",
      "Trainable params: 4,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "starting training...\n",
      "4529\n",
      "ensemble list = [ 0  1 13 15  8  9 17 19 14 12 16 10  7  4  6 18 11  5  3  2]\n",
      "\n",
      "training members = [ 0  1 13 15  8  9 17 19 14 12 16 10  7  4  6 18]\n",
      "validate members = [11  5  3  2]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 1)                 4051      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,051\n",
      "Trainable params: 4,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "starting training...\n",
      "5529\n",
      "ensemble list = [ 0  2  9 15 13 16  4 18 10 19 14  8 12  5 17  3  7  6 11  1]\n",
      "\n",
      "training members = [ 0  2  9 15 13 16  4 18 10 19 14  8 12  5 17  3]\n",
      "validate members = [ 7  6 11  1]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 1)                 4051      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,051\n",
      "Trainable params: 4,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "starting training...\n",
      "6529\n",
      "ensemble list = [ 1  2 13  6  7  3  8 10 18 19 16 17  4 11  9  5 15 14 12  0]\n",
      "\n",
      "training members = [ 1  2 13  6  7  3  8 10 18 19 16 17  4 11  9  5]\n",
      "validate members = [15 14 12  0]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 1)                 4051      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,051\n",
      "Trainable params: 4,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "starting training...\n",
      "7529\n",
      "ensemble list = [ 1  2  6  7 12 14 17  8  3 15 16 13 10  4 19 11  5  9 18  0]\n",
      "\n",
      "training members = [ 1  2  6  7 12 14 17  8  3 15 16 13 10  4 19 11]\n",
      "validate members = [ 5  9 18  0]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 1)                 4051      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,051\n",
      "Trainable params: 4,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "starting training...\n",
      "training and saving done.\n"
     ]
    }
   ],
   "source": [
    "exp_dict = {}\n",
    "\n",
    "batch_size = settings[\"batch_size\"]\n",
    "verbose = False\n",
    "ridgeVal = settings[\"ridge_param\"]\n",
    "hiddens = settings[\"hiddens\"]\n",
    "landOnly = settings[\"land_only\"]\n",
    "niter = settings[\"n_epochs\"]\n",
    "lr_here = settings[\"learning_rate\"]\n",
    "rmMean = settings[\"remove_mean\"]\n",
    "\n",
    "for model_num in np.arange(0,len(settings[\"ens_seed\"])):\n",
    "    net_seed = settings[\"net_seed\"][model_num] \n",
    "    ens_seed = settings[\"ens_seed\"][model_num] \n",
    "    print(ens_seed)\n",
    "    \n",
    "    #=====================================================================================\n",
    "    # get the data\n",
    "    rng = np.random.default_rng(ens_seed)    \n",
    "    valNum = settings[\"n_train_val_test\"][1]\n",
    "    val_member = rng.choice([0,1,2],1, replace=False)\n",
    "    train_members = rng.choice(np.arange(3,np.shape(featuresC_raw)[0]),np.shape(featuresC_raw)[0]-3, replace=False)\n",
    "    train_members = np.append(np.setdiff1d([0,1,2],[val_member]),train_members)\n",
    "    number_list = np.append(train_members,val_member)   \n",
    "\n",
    "    #-------------------\n",
    "    print('ensemble list = ' + str(number_list) + '\\n')\n",
    "    featuresC = featuresC_raw[number_list,:,:,:]\n",
    "    featuresF = featuresF_raw[number_list,:,:,:]\n",
    "\n",
    "    #-------------------\n",
    "    # train/test split\n",
    "    print('training members = ' + str(number_list[:-valNum]))\n",
    "    print('validate members = ' + str(number_list[-valNum:]))\n",
    "    featuresF_train = featuresF[:-valNum]\n",
    "    featuresC_train = featuresC[:-valNum]\n",
    "    featuresF_test = featuresF[-valNum:]\n",
    "    featuresC_test = featuresC[-valNum:]\n",
    "\n",
    "    labelsC_train = np.ones((featuresC_train.shape[0],featuresC_train.shape[1]))*0\n",
    "    labelsF_train = np.ones((featuresF_train.shape[0],featuresF_train.shape[1]))*1\n",
    "    labelsC_test = np.ones((featuresC_test.shape[0],featuresC_test.shape[1]))*0\n",
    "    labelsF_test = np.ones((featuresF_test.shape[0],featuresF_test.shape[1]))*1\n",
    "\n",
    "    #-------------------\n",
    "    # concatenate\n",
    "    features_train = np.append(featuresC_train,featuresF_train,axis=0)\n",
    "    features_test = np.append(featuresC_test,featuresF_test,axis=0)\n",
    "    labels_train = np.append(labelsC_train,labelsF_train,axis=0)\n",
    "    labels_test = np.append(labelsC_test,labelsF_test,axis=0)\n",
    "\n",
    "    #-------------------\n",
    "    # standardize\n",
    "    featuresMean = np.nanmean(features_train,axis=(0,1))\n",
    "    featuresStd = np.nanstd(features_train,axis=(0,1))\n",
    "\n",
    "    features_train = (features_train-featuresMean)/featuresStd\n",
    "    features_test = (features_test-featuresMean)/featuresStd\n",
    "    \n",
    "    #=====================================================================================    \n",
    "\n",
    "    #=========================================================================\n",
    "    #    FINALIZE THE DATA\n",
    "    #=========================================================================\n",
    "\n",
    "    # convert labels to categorical\n",
    "    y_train = tf.keras.utils.to_categorical(labels_train)\n",
    "    y_test = tf.keras.utils.to_categorical(labels_test)\n",
    "\n",
    "    # grab 2021-2030 for training\n",
    "    itime_train = np.where(time_dsc<=2097)[0]\n",
    "    X_train = features_train[:,itime_train,:,:]\n",
    "    X_test = features_test[:,itime_train,:,:]\n",
    "    y_train = y_train[:,itime_train,:]\n",
    "    y_test = y_test[:,itime_train,:]\n",
    "\n",
    "    # flatten the training and testing \n",
    "    X_train = X_train.reshape(X_train.shape[0]*X_train.shape[1],X_train.shape[2]*X_train.shape[3])\n",
    "    X_test = X_test.reshape(X_test.shape[0]*X_test.shape[1],X_test.shape[2]*X_test.shape[3])\n",
    "    y_train = y_train.reshape((y_train.shape[0]*y_train.shape[1],y_train.shape[2]))\n",
    "    y_test = y_test.reshape((y_test.shape[0]*y_test.shape[1],y_test.shape[2]))\n",
    "\n",
    "    if(landOnly==True):\n",
    "        X_train = X_train*landSeaMask\n",
    "        X_test = X_test*landSeaMask\n",
    "        landText = '_landOnly'\n",
    "    else:\n",
    "        landText = ''\n",
    "        X_train = X_train\n",
    "        X_test = X_test\n",
    "\n",
    "    if(rmMean==True):\n",
    "        print('removing the mean...')\n",
    "        X_train = X_train - np.nanmean(X_train,axis=1)[:,np.newaxis]\n",
    "        X_test = X_test - np.nanmean(X_test,axis=1)[:,np.newaxis]    \n",
    "\n",
    "    # replace NaN with zeros    \n",
    "    X_train[np.isnan(X_train)] = 0.\n",
    "    X_test[np.isnan(X_test)] = 0.\n",
    "\n",
    "    # make output predict whether from control\n",
    "    y_train = y_train[:,1:]\n",
    "    y_test = y_test[:,1:]\n",
    "    #=========================================================================\n",
    "\n",
    "    filename = settings[\"exp_name\"] + '_ensSeed' + str(ens_seed) + '_netSeed' + str(net_seed)\n",
    "\n",
    "    #=========================================================================\n",
    "    #     TRAIN THE MODEL\n",
    "    #=========================================================================\n",
    "    model = Sequential()\n",
    "    model.add(Dense(y_train.shape[1], input_shape=(X_train.shape[1],), activation='sigmoid', use_bias=True, kernel_regularizer=regularizers.l1_l2(l1=0.00, l2=ridgeVal),bias_initializer=tf.keras.initializers.RandomNormal(seed=net_seed),\n",
    "                kernel_initializer=tf.keras.initializers.RandomNormal(seed=net_seed)))\n",
    "    model.compile(optimizer=optimizers.SGD(lr=lr_here, momentum=0.9, nesterov=True),  #Adadelta .Adam()\n",
    "                 loss = 'binary_crossentropy',\n",
    "                 metrics=[metrics.categorical_accuracy],)\n",
    "    print(model.summary())\n",
    "\n",
    "    print('starting training...')\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        batch_size=batch_size, \n",
    "                        epochs=niter, \n",
    "                        shuffle=True, \n",
    "                        verbose=verbose, \n",
    "                        callbacks=[lr_callback,], \n",
    "                        validation_data=(X_test,y_test))\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    y_pred_like_train, y_pred_train, acc_train = get_predictions(X_train,labels_train,features_train)\n",
    "    y_pred_like_test, y_pred_test, acc_test = get_predictions(X_test,labels_test,features_test)\n",
    "    d = {\"y_pred_like_train\":y_pred_like_train,\n",
    "         \"y_pred_train\": y_pred_train,\n",
    "         \"acc_train\": acc_train,\n",
    "         \"y_pred_like_test\": y_pred_like_test,\n",
    "         \"y_pred_test\": y_pred_test,\n",
    "         \"acc_test\": acc_test,\n",
    "         \"filename\": filename,\n",
    "         \"exp_name\": settings[\"exp_name\"],\n",
    "         \"net_seed\": net_seed,\n",
    "         \"ens_seed\": ens_seed,\n",
    "    }\n",
    "    exp_dict[filename] = d\n",
    "    \n",
    "    #=========================================================================\n",
    "    #     SAVE OUTPUT\n",
    "    #=========================================================================\n",
    "    model.save('saved_models/model_' + filename + '.h5')\n",
    "\n",
    "\n",
    "with open(\"saved_predictions/\" + settings[\"exp_name\"] + \".pickle\", 'wb') as handle:\n",
    "    pickle.dump(exp_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('training and saving done.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
